# -*- coding: utf-8 -*-
"""notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/valeriandwi/dicoding-data-scientist-expert/blob/main/Final%20Project/notebook.ipynb

# Proyek Akhir: Menyelesaikan Permasalahan Perusahaan Edutech

- Nama: Valerian Dwi Purnomo <br/>
- Email: valerian.dwi.p@gmail.com <br/>
- Id Dicoding: valeriandwi <br/>

## Persiapan

### Menyiapkan library yang dibutuhkan
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""#Menyiapkan data yang akan digunakan
##Data Understanding
"""

institutions_df = pd.read_csv(
    "https://raw.githubusercontent.com/dicodingacademy/dicoding_dataset/main/students_performance/data.csv",
    encoding='utf-8',
    delimiter=";",
)

institutions_df.head(5)

institutions_df.isna().sum()

institutions_df.isnull().sum()

#Make sure datas type
institutions_df.info()

institutions_df.describe(include="all")

categorical_columns = [
    "Marital_status",
    "Application_mode",
    "Course",
    "Daytime_evening_attendance",
    "Previous_qualification",
    "Nacionality",
    "International",
    "Mothers_qualification",
    "Fathers_qualification",
    "Mothers_occupation",
    "Fathers_occupation",
    "Displaced",
    "Educational_special_needs",
    "Debtor",
    "Tuition_fees_up_to_date",
    "Gender",
    "Scholarship_holder",
    "International",
]


fig, ax = plt.subplots(len(categorical_columns), 1,figsize=(20,100))
for i, feature in enumerate(categorical_columns):
    sns.countplot(data=institutions_df, y=feature, hue=feature, ax=ax[i])
plt.tight_layout()
plt.show()

for column in categorical_columns:
  print(institutions_df[column].value_counts(),"\n")

numerical_columns = [
  "Admission_grade",
  "Application_order",
  "Age_at_enrollment",
  "Curricular_units_1st_sem_enrolled",
  "Curricular_units_1st_sem_evaluations",
  "Curricular_units_1st_sem_approved",
  "Curricular_units_1st_sem_grade",
  "Curricular_units_2nd_sem_enrolled",
  "Curricular_units_2nd_sem_evaluations",
  "Curricular_units_2nd_sem_approved",
  "Curricular_units_2nd_sem_grade",
  "Unemployment_rate",
  "Inflation_rate",
  "GDP",
  "Previous_qualification_grade"
]

def numerical_dis_plot(features, df, segment_feature=None, showfliers=True):
    fig, ax = plt.subplots(len(features), 1,figsize=(15,30))
    for i, feature in enumerate(features):
        if segment_feature:
            sns.boxplot(y=segment_feature, x=feature, data=df, ax=ax[i], showfliers=showfliers)
            ax[i].set_ylabel(None)
        else:
            sns.boxplot(x=feature, data=df, ax=ax[i], showfliers=showfliers)
    plt.tight_layout()
    plt.show()

numerical_dis_plot(
    features=numerical_columns,
    df=institutions_df
)

numerical_dis_plot(
    features=numerical_columns,
    df=institutions_df,
    segment_feature="Status"
)

def categorical_plot(features, df, segment_feature=None):
    fig, ax = plt.subplots(len(features), 1,figsize=(20,100))
    for i, feature in enumerate(features):
        if segment_feature:
            sns.countplot(data=df, y=segment_feature, hue=feature, ax=ax[i])
        else:
            sns.countplot(data=df, x=feature, ax=ax[i])
    plt.tight_layout()
    plt.show()

categorical_plot(
    features=categorical_columns,
    df=institutions_df,
    segment_feature="Status"
)

plt.figure(figsize=(20,20))
sns.heatmap(institutions_df[numerical_columns].corr(), annot=True, cmap='jet', linecolor='black', linewidth=1)
plt.show()

"""## Data Preparation / Preprocessing

### Train-test Split
"""

new_cleaned_df = institutions_df.drop(columns=["Curricular_units_1st_sem_credited", "Curricular_units_1st_sem_without_evaluations", "Curricular_units_2nd_sem_credited", "Curricular_units_2nd_sem_without_evaluations"],axis=1)
new_cleaned_df.head(5)

from sklearn.model_selection import train_test_split

train_df, test_df = train_test_split(new_cleaned_df, test_size=0.05, random_state=42, shuffle=True)
train_df.reset_index(drop=True, inplace=True)
test_df.reset_index(drop=True, inplace=True)

print(train_df.shape)
print(test_df.shape)

"""### Oversampling"""

sns.countplot(data=train_df, x="Status", hue="Status")
plt.show()

train_df.Status.value_counts()

df_majority_1 = train_df[(train_df.Status == "Graduate")]
df_majority_2 = train_df[(train_df.Status == "Dropout")]
df_minority = train_df[(train_df.Status == "Enrolled")]

from sklearn.utils import resample

df_majority_2_undersampled = resample(df_majority_2, n_samples=2101, random_state=42) # Oversampling
df_minority_undersampled  = resample(df_minority, n_samples=2101, random_state=42)
print(df_majority_2_undersampled.shape)
print(df_minority_undersampled.shape)

from sklearn.utils import shuffle

oversampled_train_df  = pd.concat([df_majority_1, df_majority_2_undersampled]).reset_index(drop=True)
oversampled_train_df = pd.concat([oversampled_train_df, df_minority_undersampled]).reset_index(drop=True)
oversampled_train_df = shuffle(oversampled_train_df, random_state=42)
oversampled_train_df.reset_index(drop=True, inplace=True)
oversampled_train_df.sample(5)

oversampled_train_df.Status.value_counts()

oversampled_train_df.shape

sns.countplot(data=oversampled_train_df,x="Status",hue="Status")
plt.show()

"""### Split X and Y"""

X_train = oversampled_train_df.drop(columns="Status", axis=1)
y_train = oversampled_train_df["Status"]

X_test = test_df.drop(columns="Status", axis=1)
y_test = test_df["Status"]

"""### Standarizing & Label Encoder"""

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import MinMaxScaler
import joblib

def scaling(features, df, df_test=None):
    if df_test is not None:
        df = df.copy()
        df_test = df_test.copy()
        df = df.copy()
        for feature in features:
            scaler = MinMaxScaler()
            X = np.asanyarray(df[feature])
            X = X.reshape(-1,1)
            scaler.fit(X)
            df["{}".format(feature)] = scaler.transform(X)
            joblib.dump(scaler, "./model/scaler_{}.joblib".format(feature))

            X_test = np.asanyarray(df_test[feature])
            X_test = X_test.reshape(-1,1)
            df_test["{}".format(feature)] = scaler.transform(X_test)
        return df, df_test
    else:
        df = df.copy()
        for feature in features:
            scaler = MinMaxScaler()
            X = np.asanyarray(df[feature])
            X = X.reshape(-1,1)
            scaler.fit(X)
            df["{}".format(feature)] = scaler.transform(X)
            joblib.dump(scaler, "./model/scaler_{}.joblib".format(feature))
        return df


def encoding(features, df, df_test=None):
    df_train = df.copy()

    for feature in features:
        encoder = LabelEncoder()
        encoder.fit(df_train[feature])
        df_train[feature] = encoder.transform(df_train[feature])
        joblib.dump(encoder, "./model/encoder_{}.joblib".format(feature))

        if df_test is not None:
            # Handle unseen labels in test data
            df_test[feature] = df_test[feature].apply(lambda x: x if x in encoder.classes_ else encoder.classes_[0])

    if df_test is not None:
        return df_train, df_test
    else:
        return df_train

new_train_df, new_test_df = scaling(numerical_columns, X_train, X_test)
new_train_df, new_test_df = encoding(categorical_columns, new_train_df, new_test_df)

new_train_df

encoder = LabelEncoder()
encoder.fit(y_train)
new_y_train = encoder.transform(y_train)
joblib.dump(encoder, "./model/encoder_target.joblib")

new_y_test = encoder.transform(y_test)

plt.figure(figsize=(20,20))
sns.heatmap(new_train_df[numerical_columns].corr(), annot=True, cmap='jet', linecolor='black', linewidth=1)
plt.show()

new_y_train

"""### PCA"""

len(numerical_columns)

pca_numerical_columns_1 = [
  "Curricular_units_1st_sem_enrolled",
  "Curricular_units_1st_sem_evaluations",
  "Curricular_units_1st_sem_approved",
  "Curricular_units_1st_sem_grade",
  "Curricular_units_2nd_sem_enrolled",
  "Curricular_units_2nd_sem_evaluations",
  "Curricular_units_2nd_sem_approved",
  "Curricular_units_2nd_sem_grade",
]

pca_numerical_columns_2 = [
  "Admission_grade",
  "GDP",
  "Age_at_enrollment",
  "Previous_qualification_grade"
]

train_pca_df = new_train_df.copy().reset_index(drop=True)
test_pca_df = new_test_df.copy().reset_index(drop=True)

from sklearn.decomposition import PCA

pca = PCA(n_components=len(pca_numerical_columns_1), random_state=123)
pca.fit(train_pca_df[pca_numerical_columns_1])
princ_comp = pca.transform(train_pca_df[pca_numerical_columns_1])

var_exp = pca.explained_variance_ratio_.round(3)
cum_var_exp = np.cumsum(var_exp)

plt.bar(range(len(pca_numerical_columns_1)), var_exp, alpha=0.5, align='center', label='individual explained variance')
plt.step(range(len(pca_numerical_columns_1)), cum_var_exp, where='mid', label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.show()

cum_var_exp

pca_1 = PCA(n_components=2, random_state=123)
pca_1.fit(train_pca_df[pca_numerical_columns_1])
joblib.dump(pca_1, "./model/pca_{}.joblib".format(1))
princ_comp_1 = pca_1.transform(train_pca_df[pca_numerical_columns_1])
train_pca_df[["pc1_1", "pc1_2"]] = pd.DataFrame(princ_comp_1, columns=["pc1_1", "pc1_2"])
train_pca_df.drop(columns=pca_numerical_columns_1, axis=1, inplace=True)
train_pca_df.head()

pca = PCA(n_components=len(pca_numerical_columns_2), random_state=123)
pca.fit(train_pca_df[pca_numerical_columns_2])
princ_comp = pca.transform(train_pca_df[pca_numerical_columns_2])

var_exp = pca.explained_variance_ratio_.round(3)
cum_var_exp = np.cumsum(var_exp)

plt.bar(range(len(pca_numerical_columns_2)), var_exp, alpha=0.5, align='center', label='individual explained variance')
plt.step(range(len(pca_numerical_columns_2)), cum_var_exp, where='mid', label='cumulative explained variance')
plt.ylabel('Explained variance ratio')
plt.xlabel('Principal component index')
plt.legend(loc='best')
plt.show()

cum_var_exp

pca_2 = PCA(n_components=2, random_state=123)
pca_2.fit(train_pca_df[pca_numerical_columns_2])
joblib.dump(pca_2, "./model/pca_{}.joblib".format(2))
princ_comp_2 = pca_2.transform(train_pca_df[pca_numerical_columns_2])
train_pca_df[["pc2_1", "pc2_2"]] = pd.DataFrame(princ_comp_2, columns=["pc2_1", "pc2_2"])
train_pca_df.drop(columns=pca_numerical_columns_2, axis=1, inplace=True)
train_pca_df.head()

test_princ_comp_1 = pca_1.transform(test_pca_df[pca_numerical_columns_1])
test_pca_df[["pc1_1", "pc1_2"]] = pd.DataFrame(test_princ_comp_1, columns=["pc1_1", "pc1_2"])
test_pca_df.drop(columns=pca_numerical_columns_1, axis=1, inplace=True)
test_pca_df.head()

test_princ_comp_1 = pca_2.transform(test_pca_df[pca_numerical_columns_2])
test_pca_df[["pc2_1", "pc2_2"]] = pd.DataFrame(test_princ_comp_1, columns=["pc2_1", "pc2_2"])
test_pca_df.drop(columns=pca_numerical_columns_2, axis=1, inplace=True)
test_pca_df.head()

len(train_pca_df.columns)

"""#Modelling

##Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV

log_model = LogisticRegression(random_state=123, max_iter=4000)

param_grid = {
    "penalty": ["l1","l2", "elasticnet"],
    "C": [0.01, 0.1, 1]
}

CV_log = GridSearchCV(estimator=log_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_log.fit(train_pca_df, new_y_train)
print("best parameters: ", CV_log.best_params_)
print("accuracy : ", CV_log.best_score_)

log_model = LogisticRegression(random_state=123, C=1, penalty='l2', max_iter=4000)
log_model.fit(train_pca_df, new_y_train)
joblib.dump(log_model, "./model/log_model.joblib")

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

def evaluating(y_pred, y_true):
    labels=['Enrolled','Graduate','Dropout']

    # acc = round(accuracy_score(y_pred=y_pred, y_true=y_true), 2)
    # recall = round(recall_score(y_pred=y_pred, y_true=y_true, average="micro"), 2)
    # precission = round(precision_score(y_pred=y_pred, y_true=y_true, average="micro"), 2)
    # f1 = round(f1_score(y_pred=y_pred, y_true=y_true, average="micro"), 2)
    cnf_matrix = confusion_matrix(y_pred=y_pred, y_true=y_true, labels=labels)
    confusion_matrix_df = pd.DataFrame(cnf_matrix, labels, labels)

    return confusion_matrix_df #acc, recall, precission, f1

y_pred_train = log_model.predict(train_pca_df)
y_pred_train = encoder.inverse_transform(y_pred_train)

confusion_matrix_df = evaluating(y_pred=y_pred_train, y_true=y_train)
print(classification_report(y_pred=y_pred_train, y_true=y_train))

sns.heatmap(confusion_matrix_df, annot=True, annot_kws={'size': 14}, fmt='d', cmap='YlGnBu')
plt.ylabel('True label', fontsize=15)
plt.xlabel('Predicted label', fontsize=15)
plt.show()

y_pred_test = log_model.predict(test_pca_df)
y_pred_test = encoder.inverse_transform(y_pred_test)

confusion_matrix_df = evaluating(y_pred=y_pred_test, y_true=y_test)
# print("accuracy: ", acc)
# print("recall: ", recall)
# print("precission: ", precission)
# print("F1 Score: ", f1)
print(classification_report(y_pred=y_pred_test, y_true=y_test))

sns.heatmap(confusion_matrix_df, annot=True, annot_kws={'size': 14}, fmt='d', cmap='YlGnBu')
plt.ylabel('True label', fontsize=15)
plt.xlabel('Predicted label', fontsize=15)
plt.show()

def plot_feature_importances(feature_importances, cols):
    features = pd.DataFrame(feature_importances, columns=['coef_value']).set_index(cols)
    features = features.sort_values(by='coef_value', ascending=False)
    top_features = features
    # features = features.head(20)
    plt.figure(figsize=(10, 6))
    sns.barplot(x='coef_value', y=features.index, data=features)
    plt.show()
    return top_features

plot_feature_importances(np.mean(np.abs(log_model.coef_), axis=0), train_pca_df.columns)

"""##Decision Tree"""

from sklearn.tree import DecisionTreeClassifier

tree_model = DecisionTreeClassifier(random_state=123)

param_grid = {
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [5, 6, 7, 8],
    'criterion' :['gini', 'entropy']
}

CV_tree = GridSearchCV(estimator=tree_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_tree.fit(train_pca_df, new_y_train)

print("best parameters: ", CV_tree.best_params_)
print("accuracy : ", CV_tree.best_score_)

tree_model = DecisionTreeClassifier(
    random_state=123,
    criterion='entropy',
    max_depth=7,
    max_features='sqrt'
)

tree_model.fit(train_pca_df, new_y_train)
joblib.dump(tree_model, "./model/tree_model.joblib")

y_pred_train = tree_model.predict(train_pca_df)
y_pred_train = encoder.inverse_transform(y_pred_train)

confusion_matrix_df = evaluating(y_pred=y_pred_train, y_true=y_train)
# print("accuracy: ", acc)
# print("recall: ", recall)
# print("precissiony: ", precission)
# print("F1 Score: ", f1)
print(classification_report(y_pred=y_pred_train, y_true=y_train))

sns.heatmap(confusion_matrix_df, annot=True, annot_kws={'size': 14}, fmt='d', cmap='YlGnBu')
plt.ylabel('True label', fontsize=15)
plt.xlabel('Predicted label', fontsize=15)
plt.show()

y_pred_test = tree_model.predict(test_pca_df)
y_pred_test = encoder.inverse_transform(y_pred_test)

confusion_matrix_df = evaluating(y_pred=y_pred_test, y_true=y_test)
# print("accuracy: ", acc)
# print("recall: ", recall)
# print("precissiony: ", precission)
# print("F1 Score: ", f1)
print(classification_report(y_pred=y_pred_test, y_true=y_test))

sns.heatmap(confusion_matrix_df, annot=True, annot_kws={'size': 14}, fmt='d', cmap='YlGnBu')
plt.ylabel('True label', fontsize=15)
plt.xlabel('Predicted label', fontsize=15)
plt.show()

plot_feature_importances(tree_model.feature_importances_, train_pca_df.columns)

"""## Random Forest"""

from sklearn.ensemble import RandomForestClassifier

rdf_model = RandomForestClassifier(random_state=123)

param_grid = {
    'n_estimators': [200, 500],
    'max_features': ['auto', 'sqrt', 'log2'],
    'max_depth' : [6, 7, 8],
    'criterion' :['gini', 'entropy']
}

CV_rdf = GridSearchCV(estimator=rdf_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_rdf.fit(train_pca_df, new_y_train)

print("best parameters: ", CV_rdf.best_params_)
print("accuracy : ", CV_rdf.best_score_)

rdf_model = RandomForestClassifier(
    random_state=123,
    max_depth=8,
    n_estimators=500,
    max_features='sqrt',
    criterion='gini',
    n_jobs=-1
)
rdf_model.fit(train_pca_df, new_y_train)
joblib.dump(rdf_model, "./model/rdf_model.joblib")

y_pred_train = rdf_model.predict(train_pca_df)
y_pred_train = encoder.inverse_transform(y_pred_train)

confusion_matrix_df = evaluating(y_pred=y_pred_train, y_true=y_train)
print(classification_report(y_pred=y_pred_train, y_true=y_train))

sns.heatmap(confusion_matrix_df, annot=True, annot_kws={'size': 14}, fmt='d', cmap='YlGnBu')
plt.ylabel('True label', fontsize=15)
plt.xlabel('Predicted label', fontsize=15)
plt.show()

y_pred_test = rdf_model.predict(test_pca_df)
y_pred_test = encoder.inverse_transform(y_pred_test)

confusion_matrix_df = evaluating(y_pred=y_pred_test, y_true=y_test)
print(classification_report(y_pred=y_pred_test, y_true=y_test))

sns.heatmap(confusion_matrix_df, annot=True, annot_kws={'size': 14}, fmt='d', cmap='YlGnBu')
plt.ylabel('True label', fontsize=15)
plt.xlabel('Predicted label', fontsize=15)
plt.show()

rdf_model.feature_importances_

plot_feature_importances(rdf_model.feature_importances_, train_pca_df.columns)

"""## Gradient Boosting"""

from sklearn.ensemble import GradientBoostingClassifier

gboost_model = GradientBoostingClassifier(random_state=123)

param_grid = {
    'max_depth': [5, 8],
    'n_estimators': [200, 300],
    'learning_rate': [0.01, 0.1],
    'max_features': ['auto', 'sqrt', 'log2']
}

CV_gboost = GridSearchCV(estimator=gboost_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_gboost.fit(train_pca_df, new_y_train)

print("best parameters: ", CV_gboost.best_params_)
print("accuracy : ", CV_gboost.best_score_)

gboost_model = GradientBoostingClassifier(
    random_state=123,
    learning_rate=0.1,
    max_depth=8,
    max_features= 'sqrt',
    n_estimators=300
)
gboost_model.fit(train_pca_df, new_y_train)
joblib.dump(gboost_model, "./model/gboost_model.joblib")

plot_feature_importances(gboost_model.feature_importances_, train_pca_df.columns)

y_pred_train = gboost_model.predict(train_pca_df)
y_pred_train = encoder.inverse_transform(y_pred_train)

confusion_matrix_df = evaluating(y_pred=y_pred_train, y_true=y_train)
print(classification_report(y_pred=y_pred_train, y_true=y_train))

sns.heatmap(confusion_matrix_df, annot=True, annot_kws={'size': 14}, fmt='d', cmap='YlGnBu')
plt.ylabel('True label', fontsize=15)
plt.xlabel('Predicted label', fontsize=15)
plt.show()

y_pred_test = gboost_model.predict(test_pca_df)
y_pred_test = encoder.inverse_transform(y_pred_test)

confusion_matrix_df = evaluating(y_pred=y_pred_test, y_true=y_test)
print(classification_report(y_pred=y_pred_test, y_true=y_test))

sns.heatmap(confusion_matrix_df, annot=True, annot_kws={'size': 14}, fmt='d', cmap='YlGnBu')
plt.ylabel('True label', fontsize=15)
plt.xlabel('Predicted label', fontsize=15)
plt.show()

"""## XGBClassifier"""

from xgboost import XGBClassifier

xgb_model = XGBClassifier()

param_grid = {
    'max_depth': [5, 8],
    'n_estimators': [200, 300],
    'learning_rate': [0.01, 0.1]
}

CV_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=5, n_jobs=-1)
CV_xgb.fit(train_pca_df, new_y_train)

print("best parameters: ", CV_xgb.best_params_)
print("accuracy : ", CV_xgb.best_score_)

xgb_model = XGBClassifier(
    learning_rate=0.1, max_depth=8, n_estimators=300
)
xgb_model.fit(train_pca_df, new_y_train)
joblib.dump(gboost_model, "./model/xgb_model.joblib")

plot_feature_importances(xgb_model.feature_importances_, train_pca_df.columns)

y_pred_train = xgb_model.predict(train_pca_df)
y_pred_train = encoder.inverse_transform(y_pred_train)

confusion_matrix_df = evaluating(y_pred=y_pred_train, y_true=y_train)
print(classification_report(y_pred=y_pred_train, y_true=y_train))

sns.heatmap(confusion_matrix_df, annot=True, annot_kws={'size': 14}, fmt='d', cmap='YlGnBu')
plt.ylabel('True label', fontsize=15)
plt.xlabel('Predicted label', fontsize=15)
plt.show()

y_pred_test = xgb_model.predict(test_pca_df)
y_pred_test = encoder.inverse_transform(y_pred_test)

confusion_matrix_df = evaluating(y_pred=y_pred_test, y_true=y_test)
print(classification_report(y_pred=y_pred_test, y_true=y_test))

sns.heatmap(confusion_matrix_df, annot=True, annot_kws={'size': 14}, fmt='d', cmap='YlGnBu')
plt.ylabel('True label', fontsize=15)
plt.xlabel('Predicted label', fontsize=15)
plt.show()

"""## Evaluation

Pada proses analisis data pada proyek ini, ada berbagai pertimbangan faktor yang dapat mempengaruhi klasifikasi pada siswa, antara lain :
1. Curricular_units_1st_sem_enrolled : Jumlah mata kuliah yang diambil siswa pada semester 1
2. Curricular_units_1st_sem_evaluations : Jumlah mata kuliah yang dievaulasi pada semester 1
3. Curricular_units_1st_sem_approved : Jumlah mata kuliah yang disetujui atau diakui pada semester 1
4. Curricular_units_1st_sem_grade : Hasil akademik untuk setiap mata kuliah yang diambil oleh siswa pada semester 1
5. Curricular_units_2nd_sem_enrolled : Jumlah mata kuliah yang diambil siswa pada semester 2
6. Curricular_units_2nd_sem_evaluations : Jumlah mata kuliah yang dievaulasi pada semester 2
7. Curricular_units_2nd_sem_approved : Jumlah mata kuliah yang disetujui atau diakui pada semester 2
8. Curricular_units_2nd_sem_grade : Hasil akademik untuk setiap mata kuliah yang diambil oleh siswa pada semester 2
9. Admission_grade : Nilai atau penilaian yang digunakan sebagai kriteria untuk diterima ke dalam sebuah program pendidikan
10. GDP : ukuran nilai pasar total dari semua barang dan jasa yang dihasilkan oleh sebuah negara dalam jangka waktu tertentu, biasanya dalam setahun
11. Age_at_enrollment : Usia ketika mendaftarkan diri
12. Previous_qualification_grade : Hasil akademik sebelum mendaftar

Melalui faktor tersebut, penulis menjalankan berbagai model machine learning dengan hasil accuracy sebagai berikut:
- GridSearch : 65%
- RandomForest : 76%
- GBoost : 91%
- XGBClassifier : 90%

Sehingga didapatkan bahwa model GBoost merupakan model terbaik untuk menjalankan proses klasifikasi pada proyek ini, dimana machine learning dengan model GBoost ini lebih banyak memakai faktor akademik sebagai sampel klasifikasinya.
"""

plot_feature_importances(gboost_model.feature_importances_, train_pca_df.columns)